#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwenæ¨¡å‹ä¸‹è½½å¸®åŠ©è„šæœ¬ - æ”¯æŒ4Bå’Œ8Bæ¨¡å‹ä¸‹è½½
ä½¿ç”¨é•œåƒåŠ é€Ÿä¸‹è½½ï¼Œæ”¯æŒæ ¹æ®è®¾å¤‡æƒ…å†µé€‰æ‹©åˆé€‚çš„æ¨¡å‹
"""
import os
import sys
import argparse

# è®¾ç½®é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

def download_model(model_size, use_mirror=True):
    """
    ä¸‹è½½æŒ‡å®šå¤§å°çš„Qwenæ¨¡å‹
    
    Args:
        model_size: æ¨¡å‹å¤§å°ï¼Œ'4B' æˆ– '8B'
        use_mirror: æ˜¯å¦ä½¿ç”¨é•œåƒåŠ é€Ÿ
    """
    model_configs = {
        '4B': {
            'name': 'Qwen/Qwen3-Embedding-4B',
            'size': '~8GB',
            'memory': '8-10GB',
            'description': 'ä¸­ç­‰å¤§å°ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œé€‚åˆå¤§å¤šæ•°ç”¨æˆ·'
        },
        '8B': {
            'name': 'Qwen/Qwen3-Embedding-8B', 
            'size': '~16GB',
            'memory': '16GB+',
            'description': 'æœ€å¤§æ¨¡å‹ï¼ŒMTEBæ’è¡Œæ¦œç¬¬ä¸€ï¼Œéœ€è¦æ›´å¤šèµ„æº'
        }
    }
    
    if model_size not in model_configs:
        print(f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹å¤§å° '{model_size}'ï¼Œè¯·é€‰æ‹© '4B' æˆ– '8B'")
        return False
    
    config = model_configs[model_size]
    model_name = config['name']
    
    print("=" * 80)
    print("Qwenæ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 80)
    print(f"æ¨¡å‹å¤§å°: {model_size}")
    print(f"æ¨¡å‹åç§°: {model_name}")
    print(f"æ¨¡å‹å¤§å°: {config['size']}")
    print(f"å†…å­˜éœ€æ±‚: {config['memory']}")
    print(f"æè¿°: {config['description']}")
    
    if use_mirror:
        print("ä½¿ç”¨é•œåƒ: https://hf-mirror.com")
    else:
        print("ä½¿ç”¨å®˜æ–¹æº: HuggingFace")
    print("=" * 80)
    
    try:
        from sentence_transformers import SentenceTransformer
        
        print(f"\nğŸš€ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        print("â³ è¯·è€å¿ƒç­‰å¾…ï¼Œé¦–æ¬¡ä¸‹è½½éœ€è¦è¾ƒé•¿æ—¶é—´...")
        print(f"   4Bæ¨¡å‹çº¦8GBï¼Œ8Bæ¨¡å‹çº¦16GB")
        print()
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_folder = f'./model_cache/Qwen3-Embedding-{model_size}'
        
        # ä¸‹è½½æ¨¡å‹
        model = SentenceTransformer(
            model_name,
            device='cpu',
            trust_remote_code=True,
            cache_folder=cache_folder
        )
        
        print(f"\nâœ… æ¨¡å‹ä¸‹è½½æˆåŠŸï¼")
        print(f"ğŸ“ ç¼“å­˜ä½ç½®: {cache_folder}")
        print(f"ğŸ¯ æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")
        
        return True
        
    except ImportError:
        print("âŒ é”™è¯¯: è¯·å…ˆå®‰è£…ä¾èµ–")
        print("è¿è¡Œ: pip install -r requirements.txt")
        return False
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. å°è¯•ä½¿ç”¨é­”æ³•ä¸Šç½‘")
        print("3. æ‰‹åŠ¨ä» https://hf-mirror.com/Qwen/Qwen3-Embedding-4B ä¸‹è½½")
        print("4. æ‰‹åŠ¨ä» https://hf-mirror.com/Qwen/Qwen3-Embedding-8B ä¸‹è½½")
        return False

def print_model_comparison():
    """æ‰“å°æ¨¡å‹å¯¹æ¯”ä¿¡æ¯"""
    print("=" * 80)
    print("Qwenæ¨¡å‹å¯¹æ¯”")
    print("=" * 80)
    print(f"{'æŒ‡æ ‡':<20} {'4B':<25} {'8B':<25}")
    print("-" * 80)
    print(f"{'å‚æ•°é‡':<20} {'4B':<25} {'8B':<25}")
    print(f"{'æ¨¡å‹å¤§å°':<20} {'~8GB':<25} {'~16GB':<25}")
    print(f"{'å†…å­˜éœ€æ±‚':<20} {'8-10GB':<25} {'16GB+':<25}")
    print(f"{'C-MTEBä¸­æ–‡':<20} {'72.27':<25} {'73.84':<25}")
    print(f"{'MTEBå¤šè¯­è¨€':<20} {'69.45':<25} {'70.58':<25}")
    print(f"{'ç›¸å¯¹é€Ÿåº¦':<20} {'1.4x (å¿«40%)':<25} {'1.0x (åŸºå‡†)':<25}")
    print(f"{'ä¸‹è½½æ—¶é—´':<20} {'~12åˆ†é’Ÿ':<25} {'~25åˆ†é’Ÿ':<25}")
    print("-" * 80)
    print(f"{'æ¨èåœºæ™¯':<20}")
    print(f"  4B: å¤§å¤šæ•°ç”¨æˆ·ï¼Œå¹³è¡¡æ€§èƒ½å’Œèµ„æºä½¿ç”¨")
    print(f"  8B: è¿½æ±‚æœ€ä½³æ€§èƒ½ä¸”æœ‰è¶³å¤ŸGPUå†…å­˜çš„ç”¨æˆ·")
    print("=" * 80)

def main():
    parser = argparse.ArgumentParser(description='Qwenæ¨¡å‹ä¸‹è½½å·¥å…·')
    parser.add_argument('--model', '-m', choices=['4B', '8B', 'both'], 
                       default='both', help='é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹å¤§å° (4B, 8B, both)')
    parser.add_argument('--no-mirror', action='store_true', 
                       help='ä¸ä½¿ç”¨é•œåƒï¼Œç›´æ¥ä»å®˜æ–¹æºä¸‹è½½')
    parser.add_argument('--compare', '-c', action='store_true',
                       help='æ˜¾ç¤ºæ¨¡å‹å¯¹æ¯”ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if args.compare:
        print_model_comparison()
        return
    
    use_mirror = not args.no_mirror
    
    if args.model == 'both':
        print("ğŸ“¦ å°†ä¸‹è½½4Bå’Œ8Bä¸¤ä¸ªæ¨¡å‹")
        print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥æ ¹æ®è®¾å¤‡æƒ…å†µåç»­é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ¨¡å‹\n")
        
        success_4b = download_model('4B', use_mirror)
        print("\n" + "="*50 + "\n")
        success_8b = download_model('8B', use_mirror)
        
        if success_4b and success_8b:
            print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
            print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
            print("   - å¦‚æœGPUå†…å­˜ < 12GBï¼Œå»ºè®®ä½¿ç”¨4Bæ¨¡å‹")
            print("   - å¦‚æœGPUå†…å­˜ >= 16GBï¼Œå¯ä»¥ä½¿ç”¨8Bæ¨¡å‹è·å¾—æœ€ä½³æ€§èƒ½")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    else:
        download_model(args.model, use_mirror)

if __name__ == "__main__":
    main()
